{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=0\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=25\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=50\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=75\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=100\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=125\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=150\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=175\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=200\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=225\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=250\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=275\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=300\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=325\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=350\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=375\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=400\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=425\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=450\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=475\n",
      "Fetching URL: http://www.technewsworld.com/perl/archives/tnw/?init=500\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "import urllib\n",
    "\n",
    "# Extracting 'technewsworld' post data\n",
    "extracted_text = []\n",
    "for limit in range(0, 501, 25):\n",
    "    url = 'http://www.technewsworld.com/perl/archives/tnw/?init=%s' % limit\n",
    "    print(\"Fetching URL: %s\" %url)\n",
    "    page = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page,\"html.parser\" )\n",
    "    div_block = soup.select(\"div.teaser\")\n",
    "    for index in range(len(div_block)):\n",
    "        extracted_text.append(div_block[index].text.replace(\"[More...]\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF-IDF is used to give important to the rare words than the frequently occuring words. To understand more about the TF-IDF, watch this video 'https://www.youtube.com/watch?v=4vT4fzjkGCQ'\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# max_df and min_df are used to set up threshold to the words that needs to be considered. To understand more about these attributes, read this 'https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer/35615151#35615151'\n",
    "vectorizer = TfidfVectorizer(max_df=0.75, min_df=2, stop_words='english')\n",
    "# If you print X, you will see XxY which means X posts and Y unique words found in all the posts\n",
    "X = vectorizer.fit_transform(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 994.763\n",
      "Iteration  1, inertia 509.738\n",
      "Iteration  2, inertia 509.075\n",
      "Iteration  3, inertia 508.653\n",
      "Iteration  4, inertia 508.386\n",
      "Iteration  5, inertia 508.208\n",
      "Iteration  6, inertia 508.041\n",
      "Iteration  7, inertia 507.822\n",
      "Iteration  8, inertia 507.659\n",
      "Iteration  9, inertia 507.528\n",
      "Iteration 10, inertia 507.420\n",
      "Iteration 11, inertia 507.375\n",
      "Iteration 12, inertia 507.344\n",
      "Converged at iteration 12: center shift 0.000000e+00 within tolerance 4.082702e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=4, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters = number_of_clusters, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 131],\n",
       "       [  1,  90],\n",
       "       [  2, 173],\n",
       "       [  3, 131]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import itemfreq\n",
    "\n",
    "# To check how many posts each cluster have\n",
    "itemfreq(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorising text to clusters\n",
    "text_categorised ={}\n",
    "\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    text = extracted_text[i]\n",
    "    if cluster not in text_categorised.keys():\n",
    "        text_categorised[cluster] = text\n",
    "    else:\n",
    "        text_categorised[cluster] += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Buidling 'filter words' to remove unnecessary words that causes noise in the result\n",
    "\n",
    "special_words = [\"''\", \"``\", \"--\", \"'ve\", \"'m\", \"'re\", 'g', \"'s\", 'even', \"using\", \"however\", \"year\", \"could\", 'really', 'recently', 'actually']\n",
    "filter_words = set(stopwords.words('english') + list(punctuation) + special_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenising text in each cluster and removing the 'filter words'. Finally in 'counts' frequency distribution of each word in cluster is stored and in 'keywords' top 100 words in the cluster is stored \n",
    "keywords = {}\n",
    "counts={}\n",
    "\n",
    "for cluster in range(number_of_clusters):\n",
    "    word_sent = word_tokenize(text_categorised[cluster].lower())\n",
    "    word_sent = [word for word in word_sent if ((word not in filter_words) and (not(re.match(r'^[0-9]+$', word))))]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = sorted(freq, key=freq.get, reverse=True)[:100]\n",
    "    counts[cluster] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Storing 10 unique cluster specific keywords in each cluster\n",
    "unique_keys={}\n",
    "\n",
    "for cluster in range(number_of_clusters):   \n",
    "    other_clusters_keywords = set()\n",
    "    other_clusters = list(range(0, number_of_clusters))\n",
    "    other_clusters.remove(cluster)\n",
    "    for other_cluster in other_clusters:\n",
    "        other_clusters_keywords.update(keywords[other_cluster])\n",
    "    unique = set(keywords[cluster]) - other_clusters_keywords\n",
    "#     print(\"For Cluster %s, unique keywords are %s\" %(cluster, unique))\n",
    "    unique_keys[cluster] = sorted(unique, key=counts[cluster].get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `unique_keys`, we can know that\n",
    "\n",
    "1st cluster data is about **Video Games**,\n",
    "\n",
    "2nd cluster data is about **Mobile Phones**,\n",
    "\n",
    "3rd cluster data is about **Politics**, and\n",
    "\n",
    "4th cluster data is about **Aritificial Intelligence and Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['video',\n",
       "  'app',\n",
       "  'game',\n",
       "  'twitter',\n",
       "  'nintendo',\n",
       "  'accounts',\n",
       "  'ios',\n",
       "  'gaming',\n",
       "  'videos',\n",
       "  'service'],\n",
       " 1: ['iphone',\n",
       "  'samsung',\n",
       "  'galaxy',\n",
       "  'device',\n",
       "  'smartphone',\n",
       "  'phone',\n",
       "  'pixel',\n",
       "  'display',\n",
       "  'design',\n",
       "  'touch'],\n",
       " 2: ['government',\n",
       "  'internet',\n",
       "  'trump',\n",
       "  'presidential',\n",
       "  'think',\n",
       "  'russian',\n",
       "  'companies',\n",
       "  'president',\n",
       "  'much',\n",
       "  'likely'],\n",
       " 3: ['microsoft',\n",
       "  'group',\n",
       "  'amazon',\n",
       "  'intelligence',\n",
       "  'echo',\n",
       "  'artificial',\n",
       "  'research',\n",
       "  'nvidia',\n",
       "  'ai',\n",
       "  'machine']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Classifier algorithm is used to train the obtained data\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# To check whether the data is correctly classified, we use sample_text\n",
    "sample_text = \"Machine learning is awesome\"\n",
    "test = vectorizer.transform([sample_text])\n",
    "model = GaussianNB()\n",
    "model.fit(X.todense(), km.labels_)\n",
    "model.predict(test.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
